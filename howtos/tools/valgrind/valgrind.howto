###############################################################################

\info Testing the valgrind tool suite with C++

################################################################################

Index

# 1 Introduction
# 1.1 Overview

# 2. Using and understanding the Valgrind core
#
# 2.5. Suppressing errors

# 3. Using and understanding the Valgrind core: Advanced Topics
#
# 3.2. Debugging your program using Valgrind gdbserver and GDB
#      3.2.1. Quick Start: debugging in 3 steps
#      3.2.5. Monitor command handling by the Valgrind gdbserver
#      3.2.8. Limitations of the Valgrind gdbserver
#      3.2.10. Valgrind monitor commands


# 4. Memcheck: a memory error detector
#
# 4.0 Valgrind (memcheck) quick start
#
#    2. Preparing your program
#    3. Running your program under Memcheck
#    4. Interpreting Memcheck's output
#
# 4.1. Overview
#
# 4.2. Explanation of error messages from Memcheck 
#
#	4.2.4. Illegal frees
#
# 4.6. Memcheck Monitor Commands
#
# 4.10 Errors that memcheck does not detect
#
# 4.11 Advanced memcheck


# 5. Cachegrind: a cache and branch-prediction profiler
#
# 5.2. Using Cachegrind, cg_annotate and cg_merge
#      5.2.1. Running Cachegrind
#      5.2.2. Output File
#      5.2.3. Running cg_annotate
#      5.2.4. The Output Preamble
#      5.2.5. The Global and Function-level Counts
#      5.2.6. Line-by-line Counts
#      5.2.7. Annotating Assembly Code Programs
#      5.2.8. Forking Programs
#      5.2.9. cg_annotate Warnings
#      5.2.10. Unusual Annotation Cases
#      5.2.11. Merging Profiles with cg_merge
#      5.2.12. Differencing Profiles with cg_diff
#
# 5.3. Cachegrind Command-line Options
# 5.4. cg_annotate Command-line Options
# 5.5. cg_merge Command-line Options
# 5.6. cg_diff Command-line Options
#
# 5.7. Acting on Cachegrind's Information
#
# 5.8. Simulation Details


# 6. Callgrind: a call-graph generating cache and branch prediction profiler
#
# 6.1. Overview
#    6.1.1. Functionality
#    6.1.2. Basic Usage
#
# 6.2. Advanced Usage
#    6.2.1. Multiple profiling dumps from one program run
#    6.2.2. Limiting the range of collected events
#    6.2.3. Counting global bus events
#    6.2.4. Avoiding cycles
#    6.2.5. Forking Programs
#
# 6.3. Callgrind Command-line Options
#    6.3.1. Dump creation options
#    6.3.2. Activity options
#    6.3.3. Data collection options
#    6.3.4. Cost entity separation options
#    6.3.5. Simulation options
#    6.3.6. Cache simulation options
#
# 6.4. Callgrind Monitor Commands
# 6.5. Callgrind specific client requests
# 6.6. callgrind_annotate Command-line Options
# 6.7. callgrind_control Command-line Options
#
# 6.8 Interpreting the results


# 11. SGCheck: an experimental stack and global array overrun detector

# Appendix A - Useful valgrind options to C++

# Appendix B - Advanced Topics
#
# B.1. Running valgrind with multiple processes

# Appendix K - Code examples (../code_examples)

# Appendix T - Defined Terms

# Appendix Z - References



################################################################################
# 1 Introduction
################################################################################

# 1.1 Overview

Tools:

- None (nulgrind):  'valgrind --tool=none'

  runs the code in the virtual machine without performing any
  analysis and thus has the smallest possible CPU and memory overhead
  of all tools. Since valgrind itself provides a trace back from a
  segmentation fault, the none tool provides this traceback at minimal
  overhead.

- memcheck (memory checking):

  make your programs more correct.

- cachegrind/callgrind (time profiling):

  make your programs run faster.
  \info The separate GUI KCacheGrind visualizes output from Cachegrind/callgrind

- massif (heap profiling):

  make your programs use less memory.
  \info The separate GUI massif-visualizer visualizes output from Massif.

- exp-dhat  (heap profiling):

  Analyzes how much memory is allocated and for how long as well as
  patterns of memory usage.

- helgrind/drd (thread hazard detection):

  make your multi-threaded programs more correct. 


Experimentakl tools:

- exp-sgcheck (global/stack overrun detection):

  Its functionality is complementary to that of Memcheck: SGcheck
  finds problems that Memcheck can't, and vice versa.
  \warning Some code results in false positives from this tool.[101]

- exp-bbv (extrapolating longer executions from smaller samples).

  It is useful to people doing computer architecture research and development.



################################################################################
# 2. Using and understanding the Valgrind core
################################################################################


#===============================================================================
# 2.5. Suppressing errors
#===============================================================================

1) Valgrind reads a list of errors to suppress at startup. 

2) You can modify and add to the suppressions file at your leisure,
   or, better, write your own. (see 4.4. Writing suppression
   files). Multiple suppression files are allowed.


\brief [2_3]


################################################################################
# 3. Using and understanding the Valgrind core: Advanced Topics
################################################################################

#===============================================================================
# 3.2. Debugging your program using Valgrind gdbserver and GDB
#===============================================================================

1) A program running under Valgrind is not executed directly by the
   CPU. Instead it runs on a synthetic CPU provided by Valgrind.

2) Debug in a synthetic environment too: Valgrind gdbserver.

   vgdb ("Valgrind to GDB") is a small program that is used as an
   intermediary between Valgrind and GDB or a shell. (3.2.9. vgdb
   command line options)


# 3.2.1. Quick Start: debugging in 3 steps

1) Run valgrind in a unix shell (i.e. a progma named 'prog')

   >> valgrind --vgdb=yes --vgdb-error=0 prog   #  --vgdb=yes is defaulot value

   --vgdb=yes or --vgdb=full (*1) 

   --vgdb-error=number 
    
		# tell the gdbserver only to become active once the
		# specified number of errors have been shown.
		# default = 0

2) In ANOTHER shell, start GDB:

   >> gdb prog

3) Then give the following command to GDB:

   (gdb) target remote | vgdb

   You can now debug your program ...


# see [1_1]-3.2.8. for the drawbacks of using --vgdb=full


# 3.2.5. Monitor command handling by the Valgrind gdbserver

Additional valgrind-specific functionality

1) Example (from within gdbserver)

   (gdb) monitor leak_check full reachable any

2) Example (from a shell command)

   vgdb --pid=3145 leak_check full reachable any


# 3.2.8. Limitations of the Valgrind gdbserver

1) Precision of "stop-at" commands.

  --vgdb=full, --gdb=no, --vgdb=yes 

WARNING With the option --vgdb=yes, the process might not stop at the
	exact requested instruction.
 
2) Processor registers and flags values.

When Valgrind gdbserver stops on an error, on a breakpoint or when
single stepping, registers and flags values might not be always up to
date due to the optimisations done by the Valgrind core.

     --vex-iropt-register-updates=
     
3) Breakpoints encountered multiple times.

Some instructions (e.g. x86 "rep movsb") are translated by Valgrind
using a loop. If a breakpoint is placed on such an instruction, the
breakpoint will be encountered multiple times -- once for each step of
the "implicit" loop implementing the instruction.

4) Execution of Inferior function calls by the Valgrind gdbserver.

   i.e  print (a>4)

WARNING Whilst an inferior call is running, the Valgrind tool will
	report errors as usual. If you do not want to have such errors
	stop the execution of the inferior call, you can use v.set
	vgdb-error to set a big value before the call, then manually
	reset it to its original value when the call is complete.

WARNING In a multithreaded program, all threads are continued, not
	just the thread instructed to make the inferior call.

5) Changing register values.

The Valgrind gdbserver will only modify the values of the thread's
registers when the thread is in status Runnable or Yielding.


3.2.10. Valgrind monitor commands

See [1_1] for details



################################################################################
# 4. Memcheck: a memory error detector
################################################################################

Memcheck is a memory error detector:

    Illegal read/write
    Use of uninitialized memory
    Invalid system call parameters
    Illegal frees
    Source/destination overlap
    Memory leaks


#===============================================================================
# 4.0 Valgrind (memcheck) quick start 
#===============================================================================

(valgrind.org/docs/manual/quick-start.html)


# How to launch

  valgrind --tool=memcheck EXECUTABLE-NAME

or

  valgrind EXECUTABLE-NAME  # as memcheck is the default tool	


# 2. Preparing your program

a) Compiling with Debug info
   
   Compile your program with -g  (Makefile: DEBUG_FLAGS = -g)

b) compiling with Optimization

   Using -O0 is also a good idea (Makefile: OPTIM_FLAGS = -O0 # the default)


   WARNING
   If you are planning to use Memcheck: On rare occasions, compiler
   optimisations (at -O2 and above, and sometimes -O1) have been
   observed to generate code which fools Memcheck into wrongly
   reporting uninitialised value errors, or missing uninitialised
   value errors.  (valgrind manual, 2.2. Getting started)


   With -O1 line numbers in error messages can be inaccurate, although
   generally speaking running Memcheck on code compiled at -O1 works
   fairly well, and the speed improvement compared to running -O0 is
   quite significant.

   Use of -O2 and above is not recommended as Memcheck occasionally
   reports uninitialised-value errors which don't really exist.


# 3. Running your program under Memcheck

If you normally run your program like this:

>  myprog arg1 arg2

Use this command line:

>  valgrind --leak-check=yes myprog arg1 arg2

# Memcheck is the default tool.
# The --leak-check option turns on the detailed memory leak detector.


# 4. Interpreting Memcheck's output
 
[ Example

  #include <stdlib.h>

  void f(void)
  {
     int* x = malloc(10 * sizeof(int));
     x[10] = 0;        // problem 1: heap block overrun
  }                    // problem 2: memory leak -- x not freed

  int main(void)
  {
     f();
     return 0;
  }

end example ]


4.1 memory error

1) memory error messages look like this:

  ==19182== Invalid write of size 4
  ==19182==    at 0x804838F: f (example.c:6)
  ==19182==    by 0x80483AB: main (example.c:11)
  ==19182==  Address 0x1BA45050 is 0 bytes after a block of size 40 alloc'd
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (example.c:5)
  ==19182==    by 0x80483AB: main (example.c:11)
  ...


2) It's worth fixing errors in the order they are reported, as later
   errors can be caused by earlier errors. Failing to do this is a
   common cause of difficulty with Memcheck.


# 4.2 memory leak.

1) Memory leak messages look like this:

  ==19182== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (a.c:5)
  ==19182==    by 0x80483AB: main (a.c:11)

2) The stack trace tells you where the leaked memory was allocated, (x
   in this example). Memcheck cannot tell you why the memory leaked,
   unfortunately.

3) There are several kinds of leaks; the two most important categories are:

"definitely lost" your program is leaking memory -- fix it!

"probably lost": your program is leaking memory, unless you're doing
	  	 funny things with pointers (such as moving them to
	  	 point to the middle of a heap block).


# 4.2.4. Illegal frees

1) Duplicate frees are detected by memcheck.

2) Not setting a pointer to 0 is not detected (because it is not an error [103])

  [ Example

    void fnonfree()
    {
      int * kk = new int[10];
      kk[0]=5;
      delete [] kk;

      // kk=0;        // warning: not reset to 0 not detected by memcheck
      delete [] kk;   // ok: (invalid free) detected 
    }

  end example ]

# 4.3  uninitialised values

1) Memcheck also reports uses of uninitialised values, most commonly with
   the message "Conditional jump or move depends on uninitialised value(s)".

2) It can be difficult to determine the root cause of these
   errors. Try using the --track-origins=yes to get extra information.


5. Caveats

Memcheck is not perfect; it occasionally produces false positives, and
there are mechanisms for suppressing these (see Suppressing errors in
the Valgrind User Manual). However, it is typically right 99% of the time.


#===============================================================================
# 4.1. Overview
#===============================================================================

1)  Memory errors versus memory leaks [2_1]

    a) Memory errors: are a red alert, they can destroy your program.

    b) Memory leaks: are not an urgent situation.


2) Memory leaks

   --leak-check=full
   --show-leak-kinds=all

   Valgrind categorizes leaks using these terms:

   + definitely lost: heap-allocated memory that was never freed to
     which the program no longer has a pointer. Valgrind knows that
     you once had the pointer, but have since lost track of it. This
     memory is definitely orphaned.

   + indirectly lost: heap-allocated memory that was never freed to
     which the only pointers to it also are lost. For example, if you
     orphan a linked list, the first node would be definitely lost,
     the subsequent nodes would be indirectly lost.

   + possibly lost: heap-allocated memory that was never freed to
     which valgrind cannot be sure whether there is a pointer or not.

   + still reachable: heap-allocated memory that was never freed to
     which the program still has a pointer at exit (typically this
     means a global variable points to it).


   WARNING Check whether the memory leaks exists if the execution
      	   ends early due to a fatal error.


3) Using gdb and Valgrind together

One handy Valgrind trick is the ability to drop into the debugger as
soon as it encounters a memory error (not a leak). You do this by
specifying the db-attach option when starting valgrind:

      valgrind --db-attach=yes

As soon as it encounters a memory error, Valgrind will ask you if you
would like to start up the debugger:

      ==6459== ---- Attach to debugger? --- [Return/N/n/Y/y/C/c] ----

When you exit the debugger, you will return to valgrind.

WARNING (Sometimes gdb will ask if you would like to kill the running
	program to which you should answer "no")


#===============================================================================
# 4.2. Explanation of error messages from Memcheck  [2_3]
#===============================================================================

1) \brief Most common errors [2_1]

- Invalid read/write of size X

  The program was observed to read/write X bytes of memory that was
  invalid. Common causes include accessing beyond the end of a heap
  block, accessing memory that has been freed, or accessing into an
  unallocated region such as from use of a uninitialized pointer.

- Use of uninitialised value /
  Conditional jump or move depends on uninitialised value(s)

  The program read the value of a memory location that was not
  previously written to, i.e. uses random junk. The second more
  specifically indicates the read occurred in the test expression in
  an if/for/while.

  Tip: flag --track-origins=yes

- Source and destination overlap in memcpy()

  Tip: use memmove in the src. code instead

- Invalid free() The program attempted to free a non-heap address or
  free the same block more than once.


# 4.2.1. Illegal read / Illegal write errors [2_3]
# 4.2.2. Use of uninitialised values 
# 4.2.3. Use of uninitialised or unaddressable values in system calls
# 4.2.4. Illegal frees
# 4.2.5. When a heap block is freed with an inappropriate deallocation function
# 4.2.6. Overlapping source and destination blocks
# 4.2.7. Fishy argument values
# 4.2.8. Memory leak detection

See [1_1] for details


#===============================================================================
# 4.6. Memcheck Monitor Commands
#===============================================================================

block_list

who_points_at <addr> [<len>]


See [1_1] for details


#===============================================================================
# 4.10 Errors that memcheck does not detect  [2_3]
#===============================================================================

There are a few types of memory errors that memcheck does not detect.

Reading or writing beyond arrays that are global or on the stack, for instance

  int x[10]; 
  // local, global or static

  x[10] = 1;

Try using exp-sgcheck for this sort of error.


#===============================================================================
# 4.11 Advanced memcheck [2_4]
#===============================================================================

4.11.1 Compiling with Valgrind macros.

1) Valgrind provides you with macros that allow you to actively
   control output and interact with the VM.

   i.e. VALGRIND_CHECK_MEM_IS_DEFINED(_qzz_addr,_qzz_len)

2) Used: used like the cout traces

3) The macros may return a value, either ‘directly’ from the macro as
   a status, or through inout arguments to the macro.

4) There are also macros to trigger Valgrind actions like performing a
   leak check.




4.11.2 Attaching a debugger.

       See 3.2. too

4.11.3 Using memory pools.


################################################################################
# 5. Cachegrind: a cache and branch-prediction profiler
################################################################################

1) In general, profiling tools will tell you the time spent in each
   function, inclusive and exclusive of time spent in calls to other
   functions. They may also tell you the time spent on each line of code.

2) cachegrind Vs callgrind: the main differences are that Callgrind
   has more information about the callstack whilst cachegrind gives
   more information about cache hit rates.

3) KCachegrind: When you’re using these tools, you’re likely to want
   to use the GUI that is available to browse the results.

4) Cache model example 

   MEM  <=>  L3 cache  <=>  L2 cache  <=>  L1  <=> CORE_1
                                           L1  <=> CORE_2
                                           ..............
                                           L1  <=> CORE_N


   Note.- Linux commands [104]: lscpu, 					   

5.1. Overview

1) Cachegrind simulates how your program interacts with a machine's
   cache hierarchy and (optionally) branch predictor.

2) Cache simulation [1_1]:

It simulates a machine with independent first-level instruction and
data caches (I1 and D1), backed by a unified second-level cache (L2).

For machines with more levels of cache (if detected), Cachegrind
simulates the first-level and last-level caches. The reason for this
choice is that the last-level cache has the most influence on runtime,
as it masks accesses to main memory.

Therefore, Cachegrind always refers to the I1, D1 and LL (last-level) caches.


3) Cachegrind gathers the following statistics:

   # Note.- Cache miss brief [102]
   a) instruction read miss: cause the largest delay.
   b) data read miss:  	     smaller delay
   c) data write miss: 	     the shortest delay

   Stats:

   a) I read miss:

   - I cache reads (Ir, which equals the number of instructions executed)

   - I1 cache instruction read misses (I1mr)

   - LL cache instruction read misses (ILmr).


   b) Data read miss:
   
   - D cache reads (Dr, which equals the number of memory reads)

   - D1 cache read misses (D1mr)

   - LL cache data read misses (DLmr).


   c) Data write miss:
   
   - D cache writes (Dw, which equals the number of memory writes)

   - D1 cache write misses (D1mw)

   - LL cache data write misses (DLmw).


   d) Branch prediction stats  (--branch-sim=yes)
   
   - Conditional branches executed (Bc)

   - Conditional branches mispredicted (Bcm).

   - Indirect branches executed (Bi)

   - Indirect branches mispredicted (Bim).


Note.- D1 total accesses is given by D1mr + D1mw

Note.- LL total accesses is given by ILmr + DLmr + DLmw.


4) These statistics are presented for the entire program and for each
   function in the program. You can also annotate each line of source
   code in the program with the counts that were caused directly by it.

5) On a modern machine:

   - An L1 miss will typically cost around 10 cycles,

   - An LL miss can cost as much as 200 cycles, (because this implies
     a main memory access) (see 5.7 3)

   - A mispredicted branch costs in the region of 10 to 30 cycles.

6) Also, since one instruction cache read is performed per instruction
   executed, you can find out how many instructions are executed per
   line, which can be useful for traditional profiling.


#===============================================================================
# 5.2. Using Cachegrind, cg_annotate and cg_merge
#===============================================================================

1) Compilation options for cachegrind

   a) Compiling with Debug info -g (idem memcheck)

   b) Compiling with Optimization turned on (-O)

      WARNING by contrast with normal Valgrind use, you probably do
      	      want to turn optimisation on, since you should profile
      	      your program as it will be normally run; i.e.:

	      memcheck:   OPTIM_FLAGS = -O0
	      cachegrind: OPTIM_FLAGS = -OX   X:the actual project level

2) Brief use

   a) run CACHEGRIND to gather the profiling information

   b) and then run CG_ANNOTATE to get a detailed presentation of that
      information.

   c) As an optional intermediate step, you can use CG_MERGE to sum
      together the outputs of multiple Cachegrind runs into a single
      file as input for cg_annotate.

   d) Alternatively, you can use CG_DIFF to difference the outputs of
      two Cachegrind runs into a single file which you then use as the
      input for cg_annotate.


3) Brief useful info (complete ifo at 5.7. Acting on Cachegrind's Information)

   1) The function-by-function counts are more useful to look at.

   2) The line-by-line source code annotations are much more useful.

      First: Ir numbers

   3) Firs look LL misses


   Thus, ILmr: "cg_annonate --show=ILmr --sort=ILmr"


# 5.2.1. Running Cachegrind

	valgrind --tool=cachegrind [CACHEGRIND-OPTIONS] prog

	==31751== I   refs:      27,742,716      # nº of instructions executed
	==31751== I1  misses:           276
	==31751== LLi misses:           275
	==31751== I1  miss rate:        0.0%
	==31751== LLi miss rate:        0.0%
	...

Note.- LL miss rate is computed relative to the total number of memory
       accesses, not the number of L1 misses:
       (ILmr + DLmr + DLmw) / (Ir + Dr + Dw)


# 5.2.2. Output File (profile data file)

1) Default name:  "cachegrind.out.<pid>"

WARNING The default <pid> allows correct profiling with the
	"--trace-children=yes" option of programs that spawn child
	processes.

2) Changing the default name:  –cachegrind-out-file

3) This file is not meant for human conssumption, use "cg_annotate" instead.


# 5.2.3. Running cg_annotate

  cg_annotate <filename>


TIP You can filter and sort the output with the "--threshold" and
    "--sort options"

    --threshold=<0--20>   a function is shown if it accounts for more than x% of
                          the counts of the primary sort event [0.1]


TIP annotated source: either with --auto=yes or on a file by file
    basis by passing the (fully qualified) filename as an argument.

    --auto=yes|no         annotate all source files containing functions
                          that helped reach the event count threshold [no]


INFO running kcachegrind

     >> kcachegrind cachegrind.out.13789 &


# 5.2.4. The Output Preamble

# cg_annonate output header:

--------------------------------------------------------------------------------
I1 cache:         32768 B, 64 B, 8-way associative
...
Thresholds:       1 100 100 100 100 100 100 100 100 100 100 100 100
Auto-annotation:  off
...
--------------------------------------------------------------------------------

1) Threshold: cg_annotate by default omits functions that cause very
   low counts to avoid drowning you in information. In this case,
   cg_annotate shows summaries the functions that account for 99% of
   the Ir counts; Ir is chosen as the threshold event since it is the
   primary sort event. The threshold can be adjusted with the
   --threshold option.
     

# 5.2.5. The Global and Function-level Counts

1) Then follows summary statistics for the whole program:

--------------------------------------------------------------------------------
Ir         I1mr ILmr Dr         D1mr   DLmr  Dw        D1mw   DLmw
--------------------------------------------------------------------------------
27,742,716  276  275 10,955,517 21,905 3,987 4,474,773 19,280 19,098  PROG. TOT


2) Then comes function-by-function statistics:

[ Example

--------------------------------------------------------------------------------
Ir        I1mr ILmr Dr        D1mr  DLmr  Dw        D1mw   DLmw    file:function
--------------------------------------------------------------------------------
8,821,482    5    5 2,654  1,621    73 1,794,230      0      0  getc.c:_IO_getc
5,222,023    4    4 2,004   16    12   875,959      1      1    ???:???
2,649,248    2    2 1,343  7,326 1,385         .      .      .  vg_main.c:strcmp


end example ]


file_name:function_name: Each function is identified by file:function name pair.

"???": the file name and/or function name could not be determined from
       debugging information. If most of the entries have the form
       ???:???  the program probably wasn't compiled with -g.

"."  : If a column contains only a dot it means the function never
     performs that event.


# 5.2.6. Line-by-line Counts

1) There are two ways to annotate source files:

a) all files: --auto=yes option  (see 5.4)

   WARNING auto-annotation can produce a lot of output if your program is large!

   WARNING the files chosen for auto-annotation are affected by the
   	   --sort and --threshold options.

   [ Example
     
     >> cg_annotate --auto=yes cachegrind.out.31510

     #output lines after:   -- Auto-annotated source: ... 

   end example ]


b) a concrete file: cg_annotate <cachegrind.out.???>  <source-file>

   WARNING <source-file> must be in the path in order to be detected:

   Use the -I/--include option to tell Valgrind where to look for
   source files if the filenames found from the debugging information
   aren't specific enough.
   
   [ Example
     
     >> cg_annotate cachegrind.out.31510 main.cpp
     # main.cpp not detected:  -- Auto-annotated source: ...


     >> cg_annotate cachegrind.out.31510  -I../main.cpp
     # detected:               -- User-annotated source: ...

   end example ]


2) --context option (see  5.4)

Sometimes only a small section of a source file is executed. To
minimise uninteresting output, Cachegrind only shows annotated lines
and lines within a small distance of annotated lines. Gaps are marked
with the line numbers so you know which part of a file the shown code
comes from, eg:

[ Example

      (figures and code for line 704)
      -- line 704 ----------------------------------------
      -- line 878 ----------------------------------------
      (figures and code for line 878)

end example ]

The amount of context to show around annotated lines is controlled by
the --context option.


# 5.2.7. Annotating Assembly Code Programs [1_1]
# 5.2.8. Forking Programs [1_1]
# 5.2.9. cg_annotate Warnings [1_1]
# 5.2.10. Unusual Annotation Cases [1_1]


# 5.2.11. Merging Profiles with cg_merge

  	  cg_merge -o outputfile file1 file2 file3 ...

1) Merge multiple results of cachegrind over a set of files.

2) Costs are summed on a per-function, per-line and per-instruction basis. 

See [1_1] for details.


# 5.2.12. Differencing Profiles with cg_diff

  	  >> cg_diff file1 file2 > cgdiff.out
	  >> cg_annotate cgdiff.out 


1) This is very useful if you want to measure how a change to a
   program affected its performance.

2) reads two profile files, finds the difference between them, and
   writes the results into another file in the same format.  You can
   then examine the merged results using cg_annotate <filename>.


#===============================================================================
# 5.3. Cachegrind Command-line Options
#===============================================================================

Cachegrind-specific options are:  [104]

--I1=<size>,<associativity>,<line size>   

    Specify the size, associativity and o the default output
    file, cachegrind.out.<pid>.

    The %p and %q format specifiers can be used to embed the process
    ID and/or the contents of an environment variable in the name, as
    is the case for the core option --log-file.


(*) See [102], 4.3 Cache lines
      ___________________________________
      | tag |	data block |  flag bits |
      |_________________________________|

      The data block (cache line) contains the actual data fetched
      from the main memory.


#===============================================================================
# 5.4. cg_annotate Command-line Options
#===============================================================================

--show=A,B,C [default: all, using order in cachegrind.out.<pid>]

    Specifies which events to show (and the column order). Useful if
    you want to concentrate on, for example, I cache misses
    (--show=I1mr,ILmr), or data read misses (--show=D1mr,DLmr), or LL
    data misses (--show=DLmr,DLmw). Best used in conjunction with


--sort.  --sort=A,B,C [default: order in cachegrind.out.<pid>]

    Specifies the events upon which the sorting of the
    function-by-function entries will be based.


--threshold=X [default: 0.1%]

    Sets the threshold for the function-by-function summary. A
    function is shown if it accounts for more than X% of the counts
    for the primary sort event. If auto-annotating, also affects which
    files are annotated.

    Note: thresholds can be set for more than one of the events,
    	  e.g. --sort=DLmr:1,DLmw:1


--auto=<no|yes> [default: no]

    When enabled, automatically annotates every file that is mentioned
    in the function-by-function summary that can be found. Also gives
    a list of those that couldn't be found.


--context=N [default: 8]

    Print N lines of context before and after each annotated line.
    Avoids printing large sections of source files that were not executed.
    Use a large number (e.g. 100000) to show all source lines.


-I<dir> --include=<dir> [default: none]

    Adds a directory to the list in which to search for files. Multiple
    -I/--include options can be given to add multiple directories.


# 5.5. cg_merge Command-line Options
# 5.6. cg_diff Command-line Options


#===============================================================================
# 5.7. Acting on Cachegrind's Information
#===============================================================================

1) Global cout Vs function-by-function

   The global hit/miss counts and miss rates are not that useful.
   The function-by-function counts are more useful to look at.

   WARNING
   If a function f is always inlined, counts will be attributed to the
   functions it is inlined into, rather than itself.


2) The line-by-line source code annotations are much more useful.

   In our experience, the best place to start is by looking at the Ir numbers.

   They simply measure how many instructions were executed for each
   line, and don't include any cache information, but they can still
   be very useful for identifying bottlenecks.


3) LL vs L1: we have found that LL misses are typically a much bigger
   source of slow-downs than L1 misses. (see 5.1. 5)

   (You can use --show=DLmr --sort=DLmr with cg_annotate to focus just
   on DLmr counts, for example.)


4) Looking at the Bcm and Bim misses can also be helpful. In
   particular, Bim misses are often caused by switch statements, and
   in some cases these switch statements can be replaced with
   table-driven code. (see [1_1] example)


#===============================================================================
# 5.8. Simulation Details
#===============================================================================

This section talks about details you don't need to know about in order
to use Cachegrind, but may be of interest to some people.

# 5.8.1. Cache Simulation Specifics

[1_1]


# 5.8.2. Branch Simulation Specifics

Cachegrind simulates branch predictors intended to be typical of
mainstream desktop/server processors of around 2004.


# 5.8.3. Accuracy

Valgrind's cache profiling has a number of shortcomings: [1_1]

WARNING While you shouldn't trust the results to be super-accurate,
	they should be close enough to be useful.


################################################################################
# 6. Callgrind: a call-graph generating cache and branch prediction profiler
################################################################################

# 6.1. Overview [1_1]

1) Callgrind is a profiling tool that records the call history among
   functions in a program's run as a call-graph:

   - number of instructions executed
   - their relationship to source lines
   - the caller/callee relationship between functions
   - and the numbers of such calls.
   - Optionally, cache simulation and/or branch prediction (similar to
     Cachegrind)

2) For presentation of the data, and interactive control of the
   profiling, two command line tools are provided:


callgrind_annotate

    This command reads in the profile data, and prints a sorted lists
    of functions, optionally with source annotation. (You can use
    kcachegrind or graphical visualization of the data)


callgrind_control

    This command enables you to INTERACTIVELY observe and control the
    status of a program currently running under Callgrind's control,
    without stopping the program. You can get statistics information
    as well as the current stack trace, and you can request zeroing of
    counters or dumping of profile data.


3) Practical difficulties [3_2]

   3.1) Lost of precision in floating point operations [3_2]. This is
   	due to the virtual environment where valgrind operates.

   3.2) Speed. You can mitigate this by controlling when callgrind
      	performs data collection. (se 6.2.2)


# 6.1.1. Functionality [1_1]

1) Cachegrind collects flat profile data: event counts (data reads,
   cache misses, etc.) are attributed directly to the function they
   occurred in. This cost attribution mechanism is called self or
   exclusive attribution.

2) Callgrind extends this functionality by propagating costs across
   function call boundaries. If function foo calls bar, the costs from
   bar are added into foo's costs. When applied to the program as a
   whole, this builds up a picture of so called inclusive costs, that
   is, where the cost of each function includes the costs of all
   functions it called, directly or indirectly.

3) Together with the call graph, this allows you to find the specific
   call chains starting from main in which the majority of the
   program's costs occur.

4) Caller/callee cost attribution is also useful for profiling
   functions called from multiple call sites, and where optimization
   opportunities depend on changing code in the callers, in particular
   by reducing the call count.


WARNING Callgrind's ability to detect function calls and returns
	depends on the instruction set of the platform it is run
	on. It works best on x86 and amd64,

WARNING Callgrind records the count of instructions, not the actual
	time spent in a function (I/O bottlenecks not detected).


#===============================================================================
#  6.1.2. Basic Usage
#===============================================================================

1) Compilation options for callgrind: the same as cachegrind

   a) Compiling with Debug info -g 

   b) Compiling with Optimization turned on (-O)


2) To start a profile run for a program, execute: 

 >> valgrind --tool=callgrind [callgrind options] your-program [program options]


3) Interactive info: callgrind_control

   >> callgrind_control [-e] -b   # -b: print backtrace;
   		     	  	  # -e: annotate the backtrace with event counts

4) Offline info: callgrind_annotate

   callgrind.out.<pid> is generated. The data file contains
   information about the calls made in the program among the functions
   executed, together with Instruction Read (Ir) event counts.

   To generate a function-by-function summary from the profile data file, use

   >> callgrind_annotate [options] callgrind.out.<pid>


# basic callgrind  [3_1]

  >> valgrind --tool=callgrind [--simulate-cache=yes]  main # optional cache sim
  >> callgrind_annotate --auto=yes <callgrind.out.pid>



5) callgrind_annotate's important options:

   --inclusive=yes  Instead of using exclusive cost of functions as
   		    sorting order, use and show inclusive costs.

   --tree=both 	    Interleave into the top level list of functions,
   		    information on the callers and the callees of each function.

   --auto=yes 	    to get annotated source code for all relevant functions
   		    for which the source can be found


6) callgrind_annotate's secundary options:

   --cache-sim=yes  measuring the cache behavior

   --branch-sim=yes branch prediction simulation (execc slow down by factor 2)


7) WARNING kcachegrind required usage

   If your code has a significant fraction of its cost in cycles (sets
   of functions calling each other in a recursive manner), you have to
   use KCachegrind, as callgrind_annotate currently does not do any cycle
   detection, which is important to get correct results in this case.


8) Profiling code somewhere in the middle of the run:

   --instr-atstart=no	    callgrind option

   and running, in a shell: "callgrind_control -i" on just before the
   interesting code section is executed.

   To exactly specify the code position where profiling should start,
   use the client request CALLGRIND_START_INSTRUMENTATION.


9) If you want to be able to see assembly code level annotation

   --dump-instr=yes  	  callgrind option
   --collect-jumps=yes	  callgrind option

   WARNING resulting profile data can only be viewed with KCachegrind


#===============================================================================
# 6.2. Advanced Usage
#===============================================================================

# 6.2.1. Multiple profiling dumps from one program run

Sometimes you are not interested in characteristics of a full program
run, but only of a small part of it, ... [1_1]


# 6.2.2. Limiting the range of collected events

1) you can disable event aggregation for uninteresting program parts.

2) First, there is the collection state. If this is off, no
   aggregation will be done: there is not much difference in regard to
   execution speed of Callgrind

    --instr-atstart=no.         # start mode

    CALLGRIND_TOGGLE_COLLECT;	# programatically mode change
    				#!\warning required: 
                                #          #include <valgrind/callgrind.h>

    --toggle-collect=function	# limit event collection to a specific function
    				# Recursive calls of the given function do not
				# trigger any action.

3) Second, there is the instrumentation mode in which Callgrind is
   running: The huge benefit is the much higher speed with
   instrumentation switched off.

   --instr-atstart=no.	            # start mode

   callgrind_control -i on/off      # interactively instr mode change

   CALLGRIND_START_INSTRUMENTATION; # programatically mode change
   CALLGRIND_STOP_INSTRUMENTATION;  #!\warning required: 
                                    #          #include <valgrind/callgrind.h>


# 6.2.3. Counting global bus events

For access to shared data among threads in a multithreaded code, ...


# 6.2.4. Avoiding cycles

PROBLEM: you lose the nice property to let you pinpoint the
bottlenecks by following call chains from main, guided via inclusive
cost. In addition, KCachegrind loses its ability to show interesting
parts of the call graph, as it uses inclusive costs to cut off
uninteresting areas.

\warning callgrind_annotate does not do any cycle detection at all
(see 6.1.2, 7) above)

[1_1]

# 6.2.5. Forking Programs

If your program forks, the child will inherit ... [1_1]


#===============================================================================
# 6.3. Callgrind Command-line Options
#===============================================================================

# 6.3.1. Dump creation options

--callgrind-out-file=<file>

... [1_1]


# 6.3.2. Activity options

These options specify when actions relating to event counts are to be
executed. For interactive control use callgrind_control.

--dump-every-bb=<count> [default: 0, never]

--dump-before=<function>

--zero-before=<function>

--dump-after=<function>


# 6.3.3. Data collection options

These options specify when events are to be aggregated into event
counts. Also see Limiting range of event collection.

--instr-atstart=<yes|no> [default: yes]

[1_1]


# 6.3.4. Cost entity separation options

These options specify how event counts should be attributed to
execution contexts.

--fn-skip=<function>

... [1_1]


# 6.3.5. Simulation options

--cache-sim=<yes|no> [default: no]

    Full cache simulation ...

 --branch-sim=<yes|no> [default: no]

    Specify if you want to do branch prediction simulation


# 6.3.6. Cache simulation options

[1_1]


#===============================================================================
# 6.4. Callgrind Monitor Commands
#===============================================================================

The Callgrind tool provides monitor commands handled by the Valgrind
gdbserver (see Monitor command handling by the Valgrind gdbserver).

    dump [<dump_hint>] requests to dump the profile data.

    zero requests to zero the profile data counters.

    instrumentation [on|off] requests to set (if parameter on/off is
    		    given) or get the current instrumentation state.

    status requests to print out some status information.


#===============================================================================
# 6.5. Callgrind specific client requests
#===============================================================================

Callgrind provides the following specific client requests in
callgrind.h. See that file for the exact details of their arguments.

CALLGRIND_DUMP_STATS

CALLGRIND_DUMP_STATS_AT(string)

CALLGRIND_TOGGLE_COLLECT 

CALLGRIND_START_INSTRUMENTATION

CALLGRIND_STOP_INSTRUMENTATION


#===============================================================================
# 6.6. callgrind_annotate Command-line Options
#===============================================================================

--auto=<yes|no> [default: no]


... [1_1]


#===============================================================================
# 6.7. callgrind_control Command-line Options
#===============================================================================

--instr=<on|off>

... [1_1]


#===============================================================================
# 6.8 Interpreting the results  [3_1] [3_3]
#===============================================================================

HINT Understanding the Ir counts
The Ir counts are basically the count of assembly instructions executed.
A single C statement can translate to 1, 2, or several assembly instructions.


WARNING
Callgrind records the count of instructions, not the actual time spent
in a function. If you have a program where the bottleneck is file I/O,
the costs associated with reading and writing files won't show up in
the profile, as those are not CPU-intensive tasks.


HINT The line-by-line counts are key for getting more helpful detail. 


WARNING deep analysis (assembly level)
You can even drop down to observe event counts at the assembly
level. Build the code with assembly-level debug information by editing
your Makefile to include compiler flags -Wa,--gstabs -save-temps. Then
when running callgrind, use the option --dump-instr=yes which requests
counts per assembly instruction. When annotating this output,
callgrind_annotate will now match events to assembly statements. Cool!


################################################################################
# 11. SGCheck: an experimental stack and global array overrun detector
################################################################################

1) SGCheck is a tool for finding overruns of stack and global arrays

   [ Example [2_3]

     int x[10];  // local, global or static

     x[10] = 1;  // stack array: writing out of bounds 

   end example ]

2) MEMCHECK vs SGCHECK

   SGCheck and Memcheck are complementary: their capabilities do not overlap.

3) Limitations  [1_1] [101]

   WARNING Experimental tool

   - False negatives (missed errors)

   - False positives (false errors)

   ...

   See [1_1].11.5 for details.
   

################################################################################
# Appendix A - Useful valgrind options to C++
################################################################################

# C++ compiler                # valgrind (alternative)

-g
-O0
-Wall

-fno-inline                    --read-inline-info=yes 


			       --leak-check=yes
			       --track-origins=yes
			       --undef-value-errors=yes


			       --log-file=parent.%p.log  # %p = process PID 


1) It is best to put options that are independent of the test in the
   ".valgrindrc" and to pass options that may change on the command line. [2_2]



################################################################################
# Appendix B - Advanced Topics
################################################################################

# B.1. Running valgrind with multiple processes

i.e. Analyzing a father and a children process (see [2_2] Processes and output)



################################################################################
# Appendix K - Code examples (../code_examples)
################################################################################


# source path for boost test:  "../code_examples/valgrind/"

# Init (memcheck), "../code_examples/valgrind/quick_star_example"

# callgrind, "../code_examples/valgrind/callgrind"


################################################################################
# Appendix T - Defined Terms
################################################################################


P

- Profiling (computer programming)

  In software engineering, profiling ("program profiling", "software
  profiling") is a form of dynamic program analysis that measures, for
  example, the space (memory) or time complexity of a program, the
  usage of particular instructions, or the frequency and duration of
  function calls. Most commonly, profiling information serves to aid
  program optimization.
  (https://en.wikipedia.org/wiki/Profiling_(computer_programming)




################################################################################
# Appendix Z - References
################################################################################

[1] Valgrind general doc

[1_1] Official manual - http://valgrind.org/docs/manual/index.html

[1_2] wikipedia valgrind - https://en.wikipedia.org/wiki/Valgrind


[2] memcheck-links

[2_1] Guide to memcheck - https://web.stanford.edu/class/cs107/guide_valgrind.html

[2_2] Valgrind Part 1: Introduction - https://accu.org/index.php/journals/1930

[2_3] Valgrind Part 2: Basic memcheck - https://accu.org/index.php/journals/1913

[2_4] Valgrind Part 3: Advanced memcheck - https://accu.org/index.php/journals/1905


[3] callgrind-links

[3_1] Guide to callgrind - https://web.stanford.edu/class/cs107/guide_callgrind.html

[3_2] Valgrind Part 4: Cachegrind and Callgrind - https://accu.org/index.php/journals/1886

[3_3] Callgrind, a practical example of use - https://blog.josefsson.org/2008/02/27/real-world-performance-tuning-with-callgrind/

[100] General concepts

[101] CPU cache - https://en.wikipedia.org/wiki/CPU_cache

[102] fast_cpp.howto - cpp/estudio/cpp_language/concrete_topics/fast_cpp.howto

[103] https://stackoverflow.com/questions/704466/why-doesnt-delete-set-the-pointer-to-null

[104] cache/CPU info in Linux - https://unix.stackexchange.com/questions/167038/is-there-any-way-to-know-the-size-of-l1-l2-l3-cache-and-ram-in-linux

[105] All about memory - https://www.akkadia.org/drepper/cpumemory.pdf


